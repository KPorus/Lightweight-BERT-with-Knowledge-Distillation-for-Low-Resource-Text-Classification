{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seed for reproducibility\n",
    "def set_reproducible_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_reproducible_seed()\n",
    "\n",
    "# Config\n",
    "class Config:\n",
    "    max_len = 128\n",
    "    batch_size = 8\n",
    "    epochs = 10\n",
    "    lr = 2e-4\n",
    "    num_classes = 5\n",
    "    temperature = 3.0\n",
    "    alpha = 0.7\n",
    "    beta = 0.2\n",
    "    patience = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading PubMed pcb_med20 dataset...\")\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"./data/train.txt\",\n",
    "        \"validation\": \"./data/dev.txt\",\n",
    "        \"test\": \"./data/test.txt\"\n",
    "    },\n",
    "    delimiter=\"\\t\",\n",
    "    column_names=[\"label\", \"text\"]\n",
    ")\n",
    "valid_labels = {\"BACKGROUND\", \"OBJECTIVE\", \"METHODS\", \"RESULTS\", \"CONCLUSIONS\"}\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] in valid_labels)\n",
    "label_map = {\"BACKGROUND\": 0, \"OBJECTIVE\": 1, \"METHODS\": 2, \"RESULTS\": 3, \"CONCLUSIONS\":4}\n",
    "\n",
    "def encode_label(example):\n",
    "    example[\"label\"] = label_map[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_label)\n",
    "\n",
    "# Choose tokenizer and student model architecture\n",
    "student_arch = \"roberta\"  # or \"bert\"\n",
    "\n",
    "if student_arch == \"bert\":\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    StudentModel = BertForSequenceClassification\n",
    "else:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "    StudentModel = RobertaForSequenceClassification\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=Config.max_len)\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(preprocess, batched=True)\n",
    "tokenized_validation = dataset[\"validation\"].map(preprocess, batched=True)\n",
    "tokenized_test = dataset[\"test\"].map(preprocess, batched=True)\n",
    "\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_validation.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_loader = DataLoader(tokenized_train, batch_size=Config.batch_size, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "val_loader = DataLoader(tokenized_validation, batch_size=Config.batch_size, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "test_loader = DataLoader(tokenized_test, batch_size=Config.batch_size, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "# Load teacher model\n",
    "teacher_model_path = r'G:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\best_teacher_model2.pt'\n",
    "teacher = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=Config.num_classes)\n",
    "teacher.load_state_dict(torch.load(teacher_model_path, map_location=Config.device))\n",
    "teacher.to(Config.device)\n",
    "teacher.eval()\n",
    "\n",
    "# Initialize student model\n",
    "student = StudentModel.from_pretrained(\n",
    "    \"bert-base-uncased\" if student_arch == \"bert\" else \"roberta-base\",\n",
    "    num_labels=Config.num_classes\n",
    ")\n",
    "student.to(Config.device)\n",
    "\n",
    "# Distillation loss - fixed\n",
    "def fixed_distillation_loss(student_logits, teacher_logits, labels, temperature=3.0,\n",
    "                            alpha=0.7, beta=0.2, model=None, l2_lambda=1e-5,\n",
    "                            epoch=0, max_epochs=25):\n",
    "    progress = min(epoch / max_epochs, 1.0)\n",
    "    alpha_adaptive = alpha * (1 - progress * 0.1)\n",
    "    beta_adaptive = beta * (1 - progress * 0.2)\n",
    "    gamma_adaptive = max(0.3, 1 - alpha_adaptive - beta_adaptive)\n",
    "\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "        nn.functional.log_softmax(student_logits / temperature, dim=1),\n",
    "        nn.functional.softmax(teacher_logits / temperature, dim=1)\n",
    "    )\n",
    "    hard_loss = nn.CrossEntropyLoss(label_smoothing=0.05)(student_logits, labels)\n",
    "    teacher_preds = torch.argmax(teacher_logits, dim=1)\n",
    "    sequence_loss = nn.CrossEntropyLoss(label_smoothing=0.02)(student_logits, teacher_preds)\n",
    "    l2_reg = 0\n",
    "    if model is not None:\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, 2)**2\n",
    "\n",
    "    total_loss = (alpha_adaptive * soft_loss +\n",
    "                  beta_adaptive * sequence_loss +\n",
    "                  gamma_adaptive * hard_loss +\n",
    "                  l2_lambda * l2_reg)\n",
    "    return total_loss\n",
    "\n",
    "# Training function\n",
    "def train(student, teacher, train_loader, val_loader, epochs=Config.epochs, lr=Config.lr, patience=Config.patience):\n",
    "    optimizer = optim.AdamW(student.parameters(), lr=lr)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        train_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(Config.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(Config.device)\n",
    "            labels = batch[\"label\"].to(Config.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(input_ids, attention_mask=attention_mask).logits\n",
    "            \n",
    "            student_logits = student(input_ids, attention_mask=attention_mask).logits\n",
    "            loss = fixed_distillation_loss(student_logits, teacher_logits, labels, model=student, epoch=epoch, max_epochs=epochs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(student_logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        # Validation\n",
    "        student.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(Config.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(Config.device)\n",
    "                labels = batch[\"label\"].to(Config.device)\n",
    "                teacher_logits = teacher(input_ids, attention_mask=attention_mask).logits\n",
    "                student_logits = student(input_ids, attention_mask=attention_mask).logits\n",
    "                loss = fixed_distillation_loss(student_logits, teacher_logits, labels, model=student, epoch=epoch, max_epochs=epochs)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(student_logits, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss / len(train_loader):.4f} Train Acc: {train_acc:.4f} Val Loss: {val_loss / len(val_loader):.4f} Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model = student.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping due to no improvement.\")\n",
    "                break\n",
    "    if best_model:\n",
    "        student.load_state_dict(best_model)\n",
    "    return student\n",
    "\n",
    "# Testing function\n",
    "def test(student, test_loader):\n",
    "    student.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(Config.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(Config.device)\n",
    "            labels = batch[\"label\"].to(Config.device)\n",
    "            logits = student(input_ids, attention_mask=attention_mask).logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_student = train(student, teacher, train_loader, val_loader)\n",
    "    test(trained_student, test_loader)\n",
    "    torch.save(trained_student.state_dict(), \"final_student_model.pt\")\n",
    "    print(\"Student model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
