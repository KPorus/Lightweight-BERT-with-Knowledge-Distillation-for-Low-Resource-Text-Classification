{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e377e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180040/180040 [00:38<00:00, 4703.60 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30212/30212 [00:06<00:00, 4443.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30135/30135 [00:06<00:00, 4386.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"G:/huggingface_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:/huggingface_models_cache\"\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    max_len = 128\n",
    "    freeze_n = 0  # Number of transformer encoder layers to freeze\n",
    "    batch_size = 8\n",
    "    lr = 1.04e-5\n",
    "    weight_decay = 0.001\n",
    "    dropout_rate = 0.1\n",
    "    warmup_ratio = 0.06\n",
    "    scheduler_type = 'cosine'\n",
    "    patience = 3\n",
    "    max_epochs = 5\n",
    "    save_plots = True\n",
    "    plot_dir = \"evaluation_results\"\n",
    "    output_model = \"model.pt\"\n",
    "    num_labels = 5  # Number of classes in classification\n",
    "\n",
    "class ComprehensiveEvaluator:\n",
    "    def __init__(self, save_dir=Config.plot_dir):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def plot_correlation_matrix(self, data, save=True):\n",
    "        corr = np.corrcoef(data, rowvar=False)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "        plt.title(\"Correlation Matrix\")\n",
    "        if save:\n",
    "            filename = f\"{self.save_dir}/correlation_matrix.png\"\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"ðŸ“Š Correlation matrix saved: {filename}\")\n",
    "        plt.show()\n",
    "        return corr\n",
    "\n",
    "    def plot_comprehensive_training_curves(self, history, save=True):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        epochs = range(1, len(history) + 1)\n",
    "        val_loss = [h['val_loss'] for h in history]\n",
    "        train_loss = [h['train_loss'] for h in history]\n",
    "        val_acc = [h['val_acc'] for h in history]\n",
    "        val_f1 = [h['val_f1'] for h in history]\n",
    "\n",
    "        axes[0, 0].plot(epochs, train_loss, 'b-', label='Train Loss')\n",
    "        axes[0, 0].plot(epochs, val_loss, 'r-', label='Val Loss')\n",
    "        axes[0, 0].set_title('Loss Curve')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "\n",
    "        axes[0, 1].plot(epochs, val_acc, 'r-o', label='Val Accuracy')\n",
    "        axes[0, 1].set_title('Validation Accuracy')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "\n",
    "        axes[0, 2].plot(epochs, val_f1, 'b-o', label='Val F1 Score')\n",
    "        axes[0, 2].set_title('Validation F1 Score')\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('F1 Score')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True)\n",
    "\n",
    "        if save:\n",
    "            filename = f\"{self.save_dir}/training_curves.png\"\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"ðŸ“ˆ Training curves saved to {filename}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def plot_confusion_matrix_advanced(self, y_true, y_pred, fold=0, save=True):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        class_names = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    cbar_kws={'label': 'Count'})\n",
    "        ax1.set_title(f'Confusion Matrix (Counts) - Fold {fold}')\n",
    "        ax1.set_xlabel('Predicted Label')\n",
    "        ax1.set_ylabel('True Label')\n",
    "\n",
    "        cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Reds', ax=ax2,\n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    cbar_kws={'label': 'Normalized Count'})\n",
    "        ax2.set_title(f'Normalized Confusion Matrix - Fold {fold}')\n",
    "        ax2.set_xlabel('Predicted Label')\n",
    "        ax2.set_ylabel('True Label')\n",
    "\n",
    "        fig.suptitle(f'Confusion Matrix Analysis - Fold {fold}')\n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            filename = f\"{self.save_dir}/confusion_matrix_fold_{fold}.png\"\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"ðŸ“Š Confusion matrix saved: {filename}\")\n",
    "        plt.show()\n",
    "        return fig, cm\n",
    "\n",
    "    def plot_roc_and_pr_curves(self, y_true, y_probs, fold=0, save=True):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        n_classes = y_probs.shape[1]\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs[:, i])\n",
    "            roc_auc = roc_auc_score(y_true_bin[:, i], y_probs[:, i])\n",
    "            ax1.plot(fpr, tpr, lw=2, label=f'Class {i} ROC (AUC = {roc_auc:.2f})')\n",
    "\n",
    "        ax1.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        ax1.set_xlim([0.0, 1.0])\n",
    "        ax1.set_ylim([0.0, 1.05])\n",
    "        ax1.set_title(f'ROC Curve - Fold {fold}')\n",
    "        ax1.set_xlabel('False Positive Rate')\n",
    "        ax1.set_ylabel('True Positive Rate')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_probs[:, i])\n",
    "            pr_auc = np.trapz(precision, recall)\n",
    "            ax2.plot(recall, precision, lw=2, label=f'Class {i} PR (AUC = {pr_auc:.2f})')\n",
    "\n",
    "        ax2.set_xlim([0.0, 1.0])\n",
    "        ax2.set_ylim([0.0, 1.05])\n",
    "        ax2.set_title(f'Precision-Recall Curve - Fold {fold}')\n",
    "        ax2.set_xlabel('Recall')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.legend(loc='lower left')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        fig.suptitle(f'ROC and PR Curves - Fold {fold}')\n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            filename = f\"{self.save_dir}/roc_pr_fold_{fold}.png\"\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"ðŸ“ˆ ROC/PR curves saved: {filename}\")\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def create_classification_table(self, y_true, y_pred, fold=0, save=True):\n",
    "        class_names = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\n",
    "        class_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CLASSIFICATION REPORT TABLE - Fold {fold}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        for class_name in class_names:\n",
    "            precision = class_report[class_name]['precision']\n",
    "            recall = class_report[class_name]['recall']\n",
    "            f1 = class_report[class_name]['f1-score']\n",
    "            support = int(class_report[class_name]['support'])\n",
    "            print(f\"{class_name:<15} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f} {support:<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        macro_avg = class_report['macro avg']\n",
    "        print(f\"{'Macro Avg':<15} {macro_avg['precision']:<10.4f} {macro_avg['recall']:<10.4f} {macro_avg['f1-score']:<10.4f} {int(macro_avg['support']):<10}\")\n",
    "        weighted_avg = class_report['weighted avg']\n",
    "        print(f\"{'Weighted Avg':<15} {weighted_avg['precision']:<10.4f} {weighted_avg['recall']:<10.4f} {weighted_avg['f1-score']:<10.4f} {int(weighted_avg['support']):<10}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        if save:\n",
    "            filename = f\"{self.save_dir}/classification_report_fold_{fold}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(class_report, f, indent=2)\n",
    "            print(f\"\\nðŸ’¾ Classification report saved: {filename}\")\n",
    "        return class_report\n",
    "\n",
    "    def create_comprehensive_evaluation_metrics(self, y_true, y_pred, y_probs, fold=0, save=True):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"COMPREHENSIVE EVALUATION METRICS - Fold {fold}\")\n",
    "        print(\"=\"*80)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        precision_macro, recall_macro, _, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        n_classes = y_probs.shape[1]\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))\n",
    "\n",
    "        roc_auc = 0.0\n",
    "        for i in range(n_classes):\n",
    "            try:\n",
    "                roc_auc += roc_auc_score(y_true_bin[:, i], y_probs[:, i])\n",
    "            except:\n",
    "                pass\n",
    "        roc_auc /= n_classes\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "        print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "        print(f\"ROC AUC (Macro-average): {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "        comprehensive_metrics = {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"confusion_matrix\": cm.tolist()\n",
    "        }\n",
    "\n",
    "        if save:\n",
    "            filename = f\"{self.save_dir}/comprehensive_metrics_fold_{fold}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(comprehensive_metrics, f, indent=2)\n",
    "            print(f\"ðŸ’¾ Comprehensive metrics saved: {filename}\")\n",
    "        return comprehensive_metrics\n",
    "\n",
    "def freeze_transformer_layers(model, freeze_n):\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        if i < freeze_n:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "def train_and_evaluate():\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\n",
    "            \"train\": \"./data/train.txt\",\n",
    "            \"validation\": \"./data/dev.txt\",\n",
    "            \"test\": \"./data/test.txt\"\n",
    "        },\n",
    "        delimiter=\"\\t\",\n",
    "        column_names=[\"label\", \"text\"]\n",
    "    )\n",
    "\n",
    "    valid_labels = {\"BACKGROUND\", \"OBJECTIVE\", \"METHODS\", \"RESULTS\", \"CONCLUSIONS\"}\n",
    "    label_map = {\"BACKGROUND\": 0, \"OBJECTIVE\": 1, \"METHODS\": 2, \"RESULTS\": 3, \"CONCLUSIONS\": 4}\n",
    "\n",
    "    dataset = dataset.filter(lambda x: x[\"label\"] in valid_labels)\n",
    "\n",
    "    def encode_label(example):\n",
    "        example[\"label\"] = label_map[example[\"label\"]]\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(encode_label)\n",
    "\n",
    "    model_name = \"roberta-large-mnli\"\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_fn(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=Config.max_len)\n",
    "\n",
    "    tokenized_train = dataset[\"train\"].map(preprocess_fn, batched=True)\n",
    "    tokenized_validation = dataset[\"validation\"].map(preprocess_fn, batched=True)\n",
    "    tokenized_test = dataset[\"test\"].map(preprocess_fn, batched=True)\n",
    "\n",
    "    cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "    tokenized_train.set_format(type=\"torch\", columns=cols)\n",
    "    tokenized_validation.set_format(type=\"torch\", columns=cols)\n",
    "    tokenized_test.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "    # Optimized DataLoaders with num_workers and pin_memory enabled\n",
    "    train_loader = DataLoader(tokenized_train, batch_size=Config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(tokenized_validation, batch_size=Config.batch_size, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(tokenized_test, batch_size=Config.batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "        model_name,\n",
    "        hidden_dropout_prob=Config.dropout_rate,\n",
    "        attention_probs_dropout_prob=Config.dropout_rate,\n",
    "        num_labels=Config.num_labels,\n",
    "    )\n",
    "\n",
    "    model = RobertaForSequenceClassification(config)\n",
    "    pretrained_model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "    pretrained_state_dict = pretrained_model.state_dict()\n",
    "    filtered_state_dict = {k: v for k, v in pretrained_state_dict.items() if not k.startswith(\"classifier.\")}\n",
    "    model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "    freeze_transformer_layers(model, Config.freeze_n)\n",
    "    model.to(Config.device)\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=Config.lr,\n",
    "        weight_decay=Config.weight_decay\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_loader) * Config.max_epochs\n",
    "    warmup_steps = int(Config.warmup_ratio * total_steps)\n",
    "    scheduler_fn = get_cosine_schedule_with_warmup if Config.scheduler_type == 'cosine' else get_linear_schedule_with_warmup\n",
    "    scheduler = scheduler_fn(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    evaluator = ComprehensiveEvaluator(save_dir=Config.plot_dir)\n",
    "\n",
    "    scaler = amp.GradScaler()  # Initialize GradScaler for AMP\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, Config.max_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch} Training\"):\n",
    "            inputs = {k: v.to(Config.device, non_blocking=True) for k, v in batch.items() if k != 'label'}\n",
    "            labels = batch['label'].to(Config.device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # AMP forward + backward pass\n",
    "            with amp.autocast():\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.detach().cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds, val_labels, val_probs = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {k: v.to(Config.device, non_blocking=True) for k, v in batch.items() if k != 'label'}\n",
    "                labels = batch['label'].to(Config.device, non_blocking=True)\n",
    "                with amp.autocast():\n",
    "                    outputs = model(**inputs, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}, Val F1={val_f1:.4f}\")\n",
    "        history.append({\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1\n",
    "        })\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), Config.output_model)\n",
    "            print(f\"Model saved at epoch {epoch}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= Config.patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch} due to no improvement\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(Config.output_model, map_location=Config.device))\n",
    "    model.eval()\n",
    "\n",
    "    test_preds, test_labels, test_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = {k: v.to(Config.device, non_blocking=True) for k, v in batch.items() if k != 'label'}\n",
    "            labels = batch['label'].to(Config.device, non_blocking=True)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    test_preds = np.array(test_preds)\n",
    "    test_labels = np.array(test_labels)\n",
    "    test_probs = np.array(test_probs)\n",
    "\n",
    "    sample_inputs = np.array([x['input_ids'].numpy() for x in tokenized_train[:100]])\n",
    "    evaluator.plot_correlation_matrix(sample_inputs)\n",
    "\n",
    "    evaluator.plot_comprehensive_training_curves(history)\n",
    "    evaluator.plot_confusion_matrix_advanced(test_labels, test_preds, fold=0)\n",
    "    evaluator.plot_roc_and_pr_curves(test_labels, test_probs, fold=0)\n",
    "    evaluator.create_classification_table(test_labels, test_preds, fold=0)\n",
    "    evaluator.create_comprehensive_evaluation_metrics(test_labels, test_preds, test_probs, fold=0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate()\n",
    "    \n",
    "    \n",
    "    \n",
    "# def train_and_evaluate():\n",
    "#     print(\"Loading dataset...\")\n",
    "#     dataset = load_dataset(\n",
    "#         \"csv\",\n",
    "#         data_files={\n",
    "#             \"train\": \"./data/train.txt\",\n",
    "#             \"validation\": \"./data/dev.txt\",\n",
    "#             \"test\": \"./data/test.txt\"\n",
    "#         },\n",
    "#         delimiter=\"\\t\",\n",
    "#         column_names=[\"label\", \"text\"]\n",
    "#     )\n",
    "\n",
    "#     valid_labels = {\"BACKGROUND\", \"OBJECTIVE\", \"METHODS\", \"RESULTS\", \"CONCLUSIONS\"}\n",
    "#     label_map = {\"BACKGROUND\": 0, \"OBJECTIVE\": 1, \"METHODS\": 2, \"RESULTS\": 3, \"CONCLUSIONS\": 4}\n",
    "\n",
    "#     dataset = dataset.filter(lambda x: x[\"label\"] in valid_labels)\n",
    "\n",
    "#     def encode_label(example):\n",
    "#         example[\"label\"] = label_map[example[\"label\"]]\n",
    "#         return example\n",
    "\n",
    "#     dataset = dataset.map(encode_label)\n",
    "\n",
    "#     model_name = \"roberta-large-mnli\"\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#     def preprocess_fn(examples):\n",
    "#         return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=Config.max_len)\n",
    "\n",
    "#     tokenized_train = dataset[\"train\"].map(preprocess_fn, batched=True)\n",
    "#     tokenized_validation = dataset[\"validation\"].map(preprocess_fn, batched=True)\n",
    "#     tokenized_test = dataset[\"test\"].map(preprocess_fn, batched=True)\n",
    "\n",
    "#     cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "#     tokenized_train.set_format(type=\"torch\", columns=cols)\n",
    "#     tokenized_validation.set_format(type=\"torch\", columns=cols)\n",
    "#     tokenized_test.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "#     train_loader = DataLoader(tokenized_train, batch_size=Config.batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(tokenized_validation, batch_size=Config.batch_size)\n",
    "#     test_loader = DataLoader(tokenized_test, batch_size=Config.batch_size)\n",
    "\n",
    "#     config = RobertaConfig.from_pretrained(\n",
    "#         model_name,\n",
    "#         hidden_dropout_prob=Config.dropout_rate,\n",
    "#         attention_probs_dropout_prob=Config.dropout_rate,\n",
    "#         num_labels=Config.num_labels,\n",
    "#     )\n",
    "#     # Initialize model from scratch with 5 output labels\n",
    "#     model = RobertaForSequenceClassification(config)\n",
    "\n",
    "#     # Load the pretrained checkpoint weights separately\n",
    "#     pretrained_model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "#     # Get the state dict from pretrained but exclude classifier weights\n",
    "#     pretrained_state_dict = pretrained_model.state_dict()\n",
    "#     filtered_state_dict = {k: v for k, v in pretrained_state_dict.items() if not k.startswith(\"classifier.\")}\n",
    "\n",
    "#     # Load filtered weights, strict=False allows missing classifier keys\n",
    "#     model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "#     # # Initialize model with configuration for 5 labels\n",
    "#     # model = RobertaForSequenceClassification(config)\n",
    "\n",
    "#     # # Load pretrained base weights excluding the classifier head to avoid size mismatch\n",
    "#     # state_dict = RobertaForSequenceClassification.from_pretrained(model_name, config=config).state_dict()\n",
    "#     # for key in list(state_dict.keys()):\n",
    "#     #     if \"classifier\" in key:\n",
    "#     #         del state_dict[key]\n",
    "#     # model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "#     freeze_transformer_layers(model, Config.freeze_n)\n",
    "#     model.to(Config.device)\n",
    "\n",
    "#     optimizer = optim.AdamW(\n",
    "#         filter(lambda p: p.requires_grad, model.parameters()),\n",
    "#         lr=Config.lr,\n",
    "#         weight_decay=Config.weight_decay\n",
    "#     )\n",
    "\n",
    "#     total_steps = len(train_loader) * Config.max_epochs\n",
    "#     warmup_steps = int(Config.warmup_ratio * total_steps)\n",
    "#     scheduler_fn = get_cosine_schedule_with_warmup if Config.scheduler_type == 'cosine' else get_linear_schedule_with_warmup\n",
    "#     scheduler = scheduler_fn(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "#     evaluator = ComprehensiveEvaluator(save_dir=Config.plot_dir)\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     patience_counter = 0\n",
    "#     history = []\n",
    "\n",
    "#     for epoch in range(1, Config.max_epochs + 1):\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "#         train_preds = []\n",
    "#         train_labels = []\n",
    "\n",
    "#         for batch in tqdm(train_loader, desc=f\"Epoch {epoch} Training\"):\n",
    "#             inputs = {k: v.to(Config.device) for k, v in batch.items() if k != 'label'}\n",
    "#             labels = batch['label'].to(Config.device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(**inputs, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "#             preds = torch.argmax(outputs.logits, dim=-1)\n",
    "#             train_preds.extend(preds.detach().cpu().numpy())\n",
    "#             train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#         avg_train_loss = train_loss / len(train_loader)\n",
    "#         train_acc = accuracy_score(train_labels, train_preds)\n",
    "#         train_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
    "\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_preds, val_labels, val_probs = [], [], []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 inputs = {k: v.to(Config.device) for k, v in batch.items() if k != 'label'}\n",
    "#                 labels = batch['label'].to(Config.device)\n",
    "#                 outputs = model(**inputs, labels=labels)\n",
    "#                 val_loss += outputs.loss.item()\n",
    "#                 logits = outputs.logits\n",
    "#                 probs = torch.softmax(logits, dim=-1)\n",
    "#                 preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "#                 val_preds.extend(preds.cpu().numpy())\n",
    "#                 val_labels.extend(labels.cpu().numpy())\n",
    "#                 val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "#         avg_val_loss = val_loss / len(val_loader)\n",
    "#         val_acc = accuracy_score(val_labels, val_preds)\n",
    "#         val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "#         print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}, Val F1={val_f1:.4f}\")\n",
    "#         history.append({\n",
    "#             \"train_loss\": avg_train_loss,\n",
    "#             \"train_acc\": train_acc,\n",
    "#             \"train_f1\": train_f1,\n",
    "#             \"val_loss\": avg_val_loss,\n",
    "#             \"val_acc\": val_acc,\n",
    "#             \"val_f1\": val_f1\n",
    "#         })\n",
    "\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             patience_counter = 0\n",
    "#             torch.save(model.state_dict(), Config.output_model)\n",
    "#             print(f\"Model saved at epoch {epoch}\")\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             if patience_counter >= Config.patience:\n",
    "#                 print(f\"Early stopping triggered at epoch {epoch} due to no improvement\")\n",
    "#                 break\n",
    "\n",
    "#     model.load_state_dict(torch.load(Config.output_model, map_location=Config.device))\n",
    "#     model.eval()\n",
    "\n",
    "#     test_preds, test_labels, test_probs = [], [], []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             inputs = {k: v.to(Config.device) for k, v in batch.items() if k != 'label'}\n",
    "#             labels = batch['label'].to(Config.device)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits\n",
    "#             probs = torch.softmax(logits, dim=-1)\n",
    "#             preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "#             test_preds.extend(preds.cpu().numpy())\n",
    "#             test_labels.extend(labels.cpu().numpy())\n",
    "#             test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "#     test_preds = np.array(test_preds)\n",
    "#     test_labels = np.array(test_labels)\n",
    "#     test_probs = np.array(test_probs)\n",
    "\n",
    "#     sample_inputs = np.array([x['input_ids'].numpy() for x in tokenized_train[:100]])\n",
    "#     evaluator.plot_correlation_matrix(sample_inputs)\n",
    "\n",
    "#     evaluator.plot_comprehensive_training_curves(history)\n",
    "#     evaluator.plot_confusion_matrix_advanced(test_labels, test_preds, fold=0)\n",
    "#     evaluator.plot_roc_and_pr_curves(test_labels, test_probs, fold=0)\n",
    "#     evaluator.create_classification_table(test_labels, test_preds, fold=0)\n",
    "#     evaluator.create_comprehensive_evaluation_metrics(test_labels, test_preds, test_probs, fold=0)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"in fine tuning Device: \", device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
