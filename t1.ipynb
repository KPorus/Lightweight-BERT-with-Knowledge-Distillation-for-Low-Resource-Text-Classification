{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516f0907",
   "metadata": {},
   "source": [
    "# freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba26d4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PubMed RCT dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 195040 examples [00:00, 265944.09 examples/s]\n",
      "Generating validation split: 32712 examples [00:00, 322616.77 examples/s]\n",
      "Generating test split: 32635 examples [00:00, 466219.49 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195040/195040 [00:00<00:00, 321485.84 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32712/32712 [00:00<00:00, 363465.09 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32635/32635 [00:00<00:00, 362609.54 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180040/180040 [00:08<00:00, 20059.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30212/30212 [00:01<00:00, 21779.46 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30135/30135 [00:01<00:00, 19805.60 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: {0, 1, 2, 3, 4}, size: 180040\n",
      "validation labels: {0, 1, 2, 3, 4}, size: 30212\n",
      "test labels: {0, 1, 2, 3, 4}, size: 30135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180040/180040 [00:37<00:00, 4792.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30212/30212 [00:06<00:00, 4545.32 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30135/30135 [00:06<00:00, 4467.42 examples/s]\n",
      "[I 2025-10-24 23:10:00,641] A new study created in memory with name: no-name-d10b67a0-5bc1-43ca-9d67-e17e19a5b829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': torch.Size([8]), 'input_ids': torch.Size([8, 128]), 'attention_mask': torch.Size([8, 128])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed. Avg train loss: 0.4585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 completed. Avg train loss: 0.3419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 completed. Avg train loss: 0.2970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.880147:  10%|â–ˆ         | 1/10 [1:58:28<17:46:19, 7108.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 01:08:29,427] Trial 0 finished with value: 0.8801469614722627 and parameters: {'freeze_n': 0, 'batch_size': 16, 'lr': 1.0401663679887314e-05, 'weight_decay': 0.001, 'dropout_rate': 0.1, 'warmup_ratio': 0.06, 'scheduler_type': 'cosine', 'patience': 3, 'max_epochs': 3}. Best is trial 0 with value: 0.8801469614722627.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed. Avg train loss: 0.9420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 completed. Avg train loss: 0.5512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 completed. Avg train loss: 0.5060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.880147:  20%|â–ˆâ–ˆ        | 2/10 [4:55:06<20:21:27, 9161.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 04:05:06,969] Trial 1 finished with value: 0.8530385277373229 and parameters: {'freeze_n': 0, 'batch_size': 8, 'lr': 4.192159350410978e-06, 'weight_decay': 0.01, 'dropout_rate': 0.4, 'warmup_ratio': 0.15, 'scheduler_type': 'linear', 'patience': 4, 'max_epochs': 3}. Best is trial 0 with value: 0.8801469614722627.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed. Avg train loss: 0.7458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 completed. Avg train loss: 0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 completed. Avg train loss: 0.4270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.880147:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [6:54:29<16:02:22, 8248.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 06:04:30,599] Trial 2 finished with value: 0.8573083542963061 and parameters: {'freeze_n': 0, 'batch_size': 16, 'lr': 3.5067764992972196e-05, 'weight_decay': 0.01, 'dropout_rate': 0.4, 'warmup_ratio': 0.1, 'scheduler_type': 'cosine', 'patience': 3, 'max_epochs': 3}. Best is trial 0 with value: 0.8801469614722627.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 completed. Avg train loss: 0.7142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 completed. Avg train loss: 0.4454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.880147:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [7:58:33<10:50:58, 6509.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 07:08:34,281] Trial 3 finished with value: 0.8599894081821793 and parameters: {'freeze_n': 0, 'batch_size': 32, 'lr': 4.037506188440759e-06, 'weight_decay': 0.01, 'dropout_rate': 0.3, 'warmup_ratio': 0.15, 'scheduler_type': 'cosine', 'patience': 2, 'max_epochs': 2}. Best is trial 0 with value: 0.8801469614722627.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 completed. Avg train loss: 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 completed. Avg train loss: 0.4436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.880147:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [9:57:49<9:21:53, 6742.80s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 09:07:50,202] Trial 4 finished with value: 0.8679001721170396 and parameters: {'freeze_n': 0, 'batch_size': 8, 'lr': 1.282282545480756e-06, 'weight_decay': 0.1, 'dropout_rate': 0.2, 'warmup_ratio': 0.1, 'scheduler_type': 'linear', 'patience': 2, 'max_epochs': 2}. Best is trial 0 with value: 0.8801469614722627.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 completed. Avg train loss: 0.4959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 completed. Avg train loss: 0.3559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 completed. Avg train loss: 0.3132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 completed. Avg train loss: 0.2842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.880147:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [12:36:37<8:32:38, 7689.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 11:46:38,029] Trial 5 finished with value: 0.8707096517939892 and parameters: {'freeze_n': 0, 'batch_size': 16, 'lr': 7.312171172786409e-06, 'weight_decay': 0.001, 'dropout_rate': 0.1, 'warmup_ratio': 0.1, 'scheduler_type': 'cosine', 'patience': 4, 'max_epochs': 4}. Best is trial 0 with value: 0.8801469614722627.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Avg train loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed. Avg train loss: 0.3885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed. Avg train loss: 0.3617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed. Avg train loss: 0.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed. Avg train loss: 0.3386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.880147:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [15:55:52<7:34:12, 9084.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:05:52,747] Trial 6 finished with value: 0.8785581887991527 and parameters: {'freeze_n': 0, 'batch_size': 16, 'lr': 1.5380658115982023e-06, 'weight_decay': 0.1, 'dropout_rate': 0.1, 'warmup_ratio': 0.15, 'scheduler_type': 'linear', 'patience': 3, 'max_epochs': 5}. Best is trial 0 with value: 0.8801469614722627.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 completed. Avg train loss: 1.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.880147:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [17:53:44<7:40:10, 9203.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-10-25 17:03:44,795] Trial 7 failed with parameters: {'freeze_n': 0, 'batch_size': 8, 'lr': 3.2447648098898585e-06, 'weight_decay': 0.1, 'dropout_rate': 0.4, 'warmup_ratio': 0.15, 'scheduler_type': 'linear', 'patience': 3, 'max_epochs': 4} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18808\\1657686426.py\", line 402, in create_optimized_trial\n",
      "    history, val_preds, val_labels, val_probs, evaluation_summary = train_with_advanced_monitoring(\n",
      "                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18808\\1657686426.py\", line 349, in train_with_advanced_monitoring\n",
      "    loss.backward()\n",
      "  File \"g:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\torch\\_tensor.py\", line 647, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"g:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 354, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"g:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 829, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-25 17:03:44,918] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 455\u001b[39m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m study, final_history, best_params, final_evaluation\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     study, final_history, best_params, final_evaluation = \u001b[43mrun_advanced_hyperparameter_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸŽ‰ Advanced Hyperparameter Optimization with Comprehensive Analysis Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 425\u001b[39m, in \u001b[36mrun_advanced_hyperparameter_optimization\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    419\u001b[39m os.makedirs(Config.plot_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    420\u001b[39m study = optuna.create_study(\n\u001b[32m    421\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    422\u001b[39m     sampler=TPESampler(seed=\u001b[32m42\u001b[39m),\n\u001b[32m    423\u001b[39m     pruner=optuna.pruners.MedianPruner(n_startup_trials=\u001b[32m5\u001b[39m, n_warmup_steps=\u001b[32m1\u001b[39m)\n\u001b[32m    424\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_optimized_trial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m main_plotter = AdvancedPlotter(save_dir=Config.plot_dir)\n\u001b[32m    427\u001b[39m main_plotter.plot_hyperparameter_importance(study, save=Config.save_plots)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 402\u001b[39m, in \u001b[36mcreate_optimized_trial\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    401\u001b[39m     scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m history, val_preds, val_labels, val_probs, evaluation_summary = \u001b[43mtrain_with_advanced_monitoring\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_eval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfreeze_n\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplotter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplotter\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m final_val_acc = history[-\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    407\u001b[39m final_overfitting_severity = history[-\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33moverfitting_analysis\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mseverity\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 349\u001b[39m, in \u001b[36mtrain_with_advanced_monitoring\u001b[39m\u001b[34m(model, train_loader, train_eval_loader, val_loader, optimizer, scheduler, max_epochs, freeze_n, trial_params, trial_id, plotter)\u001b[39m\n\u001b[32m    347\u001b[39m outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n\u001b[32m    348\u001b[39m loss = outputs.loss\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Config.gradient_clipping > \u001b[32m0\u001b[39m:\n\u001b[32m    351\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), Config.gradient_clipping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Lightweight BERT with Knowledge Distillation for Low-Resource Text Classification\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.roberta import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "from transformers.optimization import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_auc_score,auc, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# =============================== CONFIGURATION =================================\n",
    "class Config:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    max_len = 128\n",
    "    batch_sizes = [8, 16, 32]\n",
    "    max_epochs = 5\n",
    "    # freeze_layers = [0, 2, 4, 6, 8, 10, 12]\n",
    "    freeze_layers = [0]\n",
    "    lr_min = 1e-6\n",
    "    lr_max = 5e-5\n",
    "    weight_decay_range = [0.001, 0.01, 0.1]\n",
    "    dropout_range = [0.1, 0.2, 0.3, 0.4]\n",
    "    gradient_clipping = 1.0\n",
    "    warmup_ratio_range = [0.06, 0.1, 0.15]\n",
    "    scheduler_types = ['linear', 'cosine']\n",
    "    patience_range = [2, 3, 4]\n",
    "    min_delta = 0.001\n",
    "    output_model = \"best_teacher_finetuned.pt\"\n",
    "    save_plots = True\n",
    "    detailed_analysis = True\n",
    "    n_trials = 10\n",
    "    plot_dir = \"training_plots\"\n",
    "\n",
    "# =============================== UTILITIES ====================================\n",
    "class AdvancedEarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001, restore_best_weights=True, overfitting_patience=2, overfitting_threshold=0.05):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.overfitting_patience = overfitting_patience\n",
    "        self.overfitting_threshold = overfitting_threshold\n",
    "        self.counter = 0\n",
    "        self.overfitting_counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_weights = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_score, model, overfitting_severity='none'):\n",
    "        performance_stop, overfitting_stop = False, False\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = model.state_dict().copy()\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                performance_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = model.state_dict().copy()\n",
    "        if overfitting_severity in ['moderate', 'high']:\n",
    "            self.overfitting_counter += 1\n",
    "            if self.overfitting_counter >= self.overfitting_patience:\n",
    "                overfitting_stop = True\n",
    "        else:\n",
    "            self.overfitting_counter = 0\n",
    "        if performance_stop or overfitting_stop:\n",
    "            self.early_stop = True\n",
    "            if self.restore_best_weights and self.best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "        return self.early_stop, performance_stop, overfitting_stop\n",
    "\n",
    "class EnhancedModelAnalyzer:\n",
    "    @staticmethod\n",
    "    def detect_overfitting_advanced(train_metrics, val_metrics, history=None, threshold=0.05):\n",
    "        acc_gap = train_metrics['acc'] - val_metrics['acc']\n",
    "        loss_gap = val_metrics['loss'] - train_metrics['loss']\n",
    "        f1_gap = train_metrics['f1'] - val_metrics['f1']\n",
    "        trend_overfitting = False\n",
    "        if history and len(history) >= 3:\n",
    "            recent_val_accs = [h['val_acc'] for h in history[-3:]]\n",
    "            recent_train_accs = [h['train_acc'] for h in history[-3:]]\n",
    "            val_trend = np.polyfit(range(len(recent_val_accs)), recent_val_accs, 1)[0]\n",
    "            train_trend = np.polyfit(range(len(recent_train_accs)), recent_train_accs, 1)[0]\n",
    "            if train_trend > 0.001 and val_trend < -0.001:\n",
    "                trend_overfitting = True\n",
    "        if acc_gap > 0.15 or loss_gap > 0.3 or trend_overfitting:\n",
    "            severity = 'high'\n",
    "        elif acc_gap > 0.1 or loss_gap > 0.2:\n",
    "            severity = 'moderate'\n",
    "        elif acc_gap > threshold or loss_gap > 0.1:\n",
    "            severity = 'low'\n",
    "        else:\n",
    "            severity = 'none'\n",
    "        return {\n",
    "            'accuracy_gap': acc_gap,\n",
    "            'loss_gap': loss_gap,\n",
    "            'f1_gap': f1_gap,\n",
    "            'trend_overfitting': trend_overfitting,\n",
    "            'is_overfitting': acc_gap > threshold or loss_gap > 0.1 or trend_overfitting,\n",
    "            'severity': severity,\n",
    "            'recommendation': EnhancedModelAnalyzer._get_enhanced_recommendation(severity, acc_gap, loss_gap, trend_overfitting)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_enhanced_recommendation(severity, acc_gap, loss_gap, trend_overfitting):\n",
    "        if severity == 'high':\n",
    "            recs = [\"Strong overfitting detected!\"]\n",
    "            if trend_overfitting:\n",
    "                recs.append(\"Validation performance declining while training improves.\")\n",
    "            recs.extend([\"Immediate actions: Stop training, increase regularization\",\n",
    "                         \"Next trial: Higher weight decay, more dropout, fewer epochs\"])\n",
    "            return \" \".join(recs)\n",
    "        elif severity == 'moderate':\n",
    "            return f\"Moderate overfitting (acc_gap: {acc_gap:.3f}). Consider: early stopping, increase dropout to 0.3-0.4\"\n",
    "        elif severity == 'low':\n",
    "            return f\"Minor overfitting (acc_gap: {acc_gap:.3f}). Monitor closely, consider slight regularization increase\"\n",
    "        else:\n",
    "            return \"Good generalization. Model performing well.\"\n",
    "\n",
    "# ============================== PLOTTING ======================================\n",
    "class AdvancedPlotter:\n",
    "    def __init__(self, save_dir=\"training_plots\"):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, class_names=None, filename=\"confusion_matrix.png\"):\n",
    "        plt.figure(figsize=(8,6))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.save_dir, filename))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_normalized_confusion_matrix(self, y_true, y_pred, class_names=None, filename=\"confusion_matrix_normalized.png\"):\n",
    "        plt.figure(figsize=(8,6))\n",
    "        cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Reds\", cbar=False,\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.title(\"Normalized Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.save_dir, filename))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_roc_curve(self, y_true, y_prob, filename=\"roc_curve.png\"):\n",
    "        # Determine number of classes from prediction probabilities shape\n",
    "        n_classes = y_prob.shape[1]\n",
    "        # Binarize true labels for multi-class ROC computation\n",
    "        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc:.3f})\")\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], \"k--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Multi-class ROC Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.save_dir, filename))\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_pr_curve(self, y_true, y_prob, filename=\"precision_recall_curve.png\"):\n",
    "        n_classes = y_prob.shape[1]\n",
    "        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "        plt.figure(figsize=(8,6))\n",
    "        for i in range(n_classes):\n",
    "            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "        plt.plot(recall, precision, lw=2, label=f'class {i}')\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(\"Precision-Recall Curve (One-vs-Rest)\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.save_dir, filename))\n",
    "        plt.close()\n",
    "\n",
    "    def create_comprehensive_evaluation_report(self, y_true, y_pred, y_prob, trial_params, history, trial_id=None, save=True):\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "        metrics = {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_weighted\": f1,\n",
    "            \"roc_auc\": auc,\n",
    "            \"classification_report\": report,\n",
    "            \"trial_params\": trial_params,\n",
    "            \"history\": history\n",
    "        }\n",
    "        if save:\n",
    "            base_filename = f\"trial_{trial_id}\" if trial_id is not None else \"final\"\n",
    "            # Save JSON metrics\n",
    "            with open(os.path.join(self.save_dir, f\"{base_filename}_metrics.json\"), \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2)\n",
    "            # Save plots\n",
    "            class_names = ['Class 0', 'Class 1']\n",
    "            self.plot_confusion_matrix(y_true, y_pred, class_names, filename=f\"{base_filename}_confusion.png\")\n",
    "            self.plot_normalized_confusion_matrix(y_true, y_pred, class_names, filename=f\"{base_filename}_confusion_normalized.png\")\n",
    "            self.plot_roc_curve(y_true, y_prob, filename=f\"{base_filename}_roc.png\")\n",
    "            self.plot_pr_curve(y_true, y_prob, filename=f\"{base_filename}_pr.png\")\n",
    "        return metrics\n",
    "\n",
    "# =============== DATASET AND TRAINING MODULES (summarized) ====================\n",
    "print(\"Loading PubMed RCT dataset...\")\n",
    "dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\n",
    "            \"train\": \"./data/train.txt\",\n",
    "            \"validation\": \"./data/dev.txt\",\n",
    "            \"test\": \"./data/test.txt\"\n",
    "        },\n",
    "        delimiter=\"\\t\",\n",
    "        column_names=[\"label\", \"text\"]\n",
    "    )\n",
    "# Filter only valid labels\n",
    "valid_labels = {\"BACKGROUND\", \"OBJECTIVE\", \"METHODS\", \"RESULTS\", \"CONCLUSIONS\"}\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] in valid_labels)\n",
    "\n",
    "# Map string labels to integer indices\n",
    "label_map = {\"BACKGROUND\": 0, \"OBJECTIVE\": 1, \"METHODS\": 2, \"RESULTS\": 3, \"CONCLUSIONS\": 4}\n",
    "def encode_label(example):\n",
    "    example[\"label\"] = label_map[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_label)\n",
    "# Print diagnostics\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    labels = set(dataset[split][\"label\"])\n",
    "    print(f\"{split} labels: {labels}, size: {len(dataset[split])}\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def preprocess(examples):\n",
    "    # Tokenization of the text column\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=Config.max_len)\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(preprocess, batched=True)\n",
    "tokenized_validation = dataset[\"validation\"].map(preprocess, batched=True)\n",
    "tokenized_test = dataset[\"test\"].map(preprocess, batched=True)\n",
    "\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_validation.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_loader = DataLoader(tokenized_train, batch_size=8)\n",
    "for batch in train_loader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break\n",
    "\n",
    "# train_loader = DataLoader(tokenized_train, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "# val_loader = DataLoader(tokenized_validation, batch_size=params[\"batch_size\"])\n",
    "# test_loader = DataLoader(tokenized_test, batch_size=params[\"batch_size\"])\n",
    "\n",
    "\n",
    "\n",
    "# def preprocess(example):\n",
    "#     encoding = tokenizer(\n",
    "#         example[\"text\"],\n",
    "#         max_length=Config.max_len,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     return {\n",
    "#         \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "#         \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "#         \"label\": example[\"label\"]\n",
    "#     }\n",
    "\n",
    "# train_dataset = dataset[\"train\"].map(preprocess)\n",
    "# val_dataset = dataset[\"test\"].map(preprocess)\n",
    "# train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "train_eval_indices = np.random.choice(len(tokenized_train), size=min(5000, len(tokenized_train)), replace=False)\n",
    "train_eval_dataset = tokenized_train.select(train_eval_indices)\n",
    "\n",
    "def freeze_transformer_layers(model, freeze_n, freeze_embed=False, freeze_classifier=False):\n",
    "    backbone = model.roberta\n",
    "    for param in backbone.embeddings.parameters():\n",
    "        param.requires_grad = not freeze_embed\n",
    "    for i, layer in enumerate(backbone.encoder.layer):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = (i >= freeze_n)\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = not freeze_classifier\n",
    "\n",
    "def comprehensive_evaluate(model, dataloader, device, return_predictions=False):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_loss += loss.item()\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    precision, recall, f1_macro, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "    metrics = {'loss': total_loss / len(dataloader), 'acc': acc, 'f1': f1, 'f1_macro': f1_macro, 'precision': precision, 'recall': recall}\n",
    "    if return_predictions:\n",
    "        return metrics, all_preds, all_labels, np.array(all_probs)\n",
    "    else:\n",
    "        return metrics\n",
    "\n",
    "def train_with_advanced_monitoring(model, train_loader, train_eval_loader, val_loader, optimizer, scheduler, max_epochs, freeze_n, trial_params, trial_id=None, plotter=None):\n",
    "    history = []\n",
    "    early_stopping = AdvancedEarlyStopping(patience=trial_params['patience'], min_delta=Config.min_delta, overfitting_patience=2, overfitting_threshold=0.05)\n",
    "    analyzer = EnhancedModelAnalyzer()\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0; num_batches = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"[Freeze {freeze_n}] Epoch {epoch+1}/{max_epochs}\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(Config.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(Config.device)\n",
    "            labels = batch[\"label\"].to(Config.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            if Config.gradient_clipping > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), Config.gradient_clipping)\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        avg_train_loss = epoch_train_loss / num_batches\n",
    "        train_metrics = comprehensive_evaluate(model, train_eval_loader, Config.device)\n",
    "        val_metrics, val_preds, val_labels, val_probs = comprehensive_evaluate(model, val_loader, Config.device, return_predictions=True)\n",
    "        overfitting_analysis = analyzer.detect_overfitting_advanced(train_metrics, val_metrics, history, threshold=0.05)\n",
    "        epoch_info = {'epoch': epoch + 1, 'train_loss': avg_train_loss, 'train_acc': train_metrics['acc'], 'train_f1': train_metrics['f1'],\n",
    "                      'val_loss': val_metrics['loss'], 'val_acc': val_metrics['acc'], 'val_f1': val_metrics['f1'], 'overfitting_analysis': overfitting_analysis}\n",
    "        history.append(epoch_info)\n",
    "        early_stop, perf_stop, overfit_stop = early_stopping(val_metrics['acc'], model, overfitting_analysis['severity'])\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs} completed. Avg train loss: {epoch_train_loss / num_batches:.4f}\")\n",
    "        if early_stop:\n",
    "            break\n",
    "    evaluation_summary = None\n",
    "    if plotter:\n",
    "        evaluation_summary = plotter.create_comprehensive_evaluation_report(val_labels, val_preds, val_probs, trial_params, history, trial_id, save=Config.save_plots)\n",
    "    return history, val_preds, val_labels, val_probs, evaluation_summary\n",
    "\n",
    "def create_optimized_trial(trial):\n",
    "    params = {\n",
    "        'freeze_n': trial.suggest_categorical('freeze_n', Config.freeze_layers),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', Config.batch_sizes),\n",
    "        'lr': trial.suggest_float('lr', Config.lr_min, Config.lr_max, log=True),\n",
    "        'weight_decay': trial.suggest_categorical('weight_decay', Config.weight_decay_range),\n",
    "        'dropout_rate': trial.suggest_categorical('dropout_rate', Config.dropout_range),\n",
    "        'warmup_ratio': trial.suggest_categorical('warmup_ratio', Config.warmup_ratio_range),\n",
    "        'scheduler_type': trial.suggest_categorical('scheduler_type', Config.scheduler_types),\n",
    "        'patience': trial.suggest_categorical('patience', Config.patience_range),\n",
    "        'max_epochs': trial.suggest_int('max_epochs', 2, Config.max_epochs)\n",
    "    }\n",
    "    plotter = AdvancedPlotter(save_dir=f\"{Config.plot_dir}/trial_{trial.number}\")\n",
    "    train_loader = DataLoader(tokenized_train, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(tokenized_validation, batch_size=params['batch_size'])\n",
    "    train_eval_loader = DataLoader(train_eval_dataset, batch_size=params['batch_size'])\n",
    "    config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "    config.hidden_dropout_prob = params['dropout_rate']\n",
    "    config.attention_probs_dropout_prob = params['dropout_rate']\n",
    "    config.num_labels = 5\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config=config).to(Config.device)\n",
    "    freeze_transformer_layers(model, params['freeze_n'])\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    total_steps = len(train_loader) * params['max_epochs']\n",
    "    warmup_steps = int(params['warmup_ratio'] * total_steps)\n",
    "    if params['scheduler_type'] == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    else:\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    history, val_preds, val_labels, val_probs, evaluation_summary = train_with_advanced_monitoring(\n",
    "        model, train_loader, train_eval_loader, val_loader, optimizer, scheduler, params['max_epochs'], params['freeze_n'],\n",
    "        params, trial_id=trial.number, plotter=plotter\n",
    "    )\n",
    "    final_val_acc = history[-1]['val_acc']\n",
    "    final_overfitting_severity = history[-1]['overfitting_analysis']['severity']\n",
    "    overfitting_penalty = {'none': 0.0, 'low': 0.01, 'moderate': 0.03, 'high': 0.05}.get(final_overfitting_severity, 0.0)\n",
    "    final_score = final_val_acc - overfitting_penalty\n",
    "    trial.report(final_score, len(history))\n",
    "    trial.set_user_attr('final_val_acc', final_val_acc)\n",
    "    trial.set_user_attr('overfitting_severity', final_overfitting_severity)\n",
    "    trial.set_user_attr('epochs_trained', len(history))\n",
    "    trial.set_user_attr('early_stopped', len(history) < params['max_epochs'])\n",
    "    trial.set_user_attr('evaluation_summary', evaluation_summary)\n",
    "    return final_score\n",
    "\n",
    "def run_advanced_hyperparameter_optimization():\n",
    "    os.makedirs(Config.plot_dir, exist_ok=True)\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n",
    "    )\n",
    "    study.optimize(create_optimized_trial, n_trials=Config.n_trials, show_progress_bar=True)\n",
    "    main_plotter = AdvancedPlotter(save_dir=Config.plot_dir)\n",
    "    main_plotter.plot_hyperparameter_importance(study, save=Config.save_plots)\n",
    "    best_params = study.best_params\n",
    "    final_plotter = AdvancedPlotter(save_dir=f\"{Config.plot_dir}/final_model\")\n",
    "    train_loader = DataLoader(tokenized_train, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(tokenized_validation, batch_size=best_params['batch_size'])\n",
    "    train_eval_loader = DataLoader(train_eval_dataset, batch_size=best_params['batch_size'])\n",
    "    config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "    config.hidden_dropout_prob = best_params['dropout_rate']\n",
    "    config.attention_probs_dropout_prob = best_params['dropout_rate']\n",
    "    config.num_labels = 5\n",
    "    final_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config=config).to(Config.device)\n",
    "    freeze_transformer_layers(final_model, best_params['freeze_n'])\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, final_model.parameters()), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "    total_steps = len(train_loader) * best_params['max_epochs']\n",
    "    warmup_steps = int(best_params['warmup_ratio'] * total_steps)\n",
    "    if best_params['scheduler_type'] == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    else:\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    final_history, final_val_preds, final_val_labels, final_val_probs, final_evaluation = train_with_advanced_monitoring(\n",
    "        final_model, train_loader, train_eval_loader, val_loader, optimizer, scheduler, best_params['max_epochs'],\n",
    "        best_params['freeze_n'], best_params, trial_id=\"FINAL\", plotter=final_plotter)\n",
    "    torch.save(final_model.state_dict(), Config.output_model)\n",
    "    print(f\"ðŸ’¾ Final optimized model saved to: {Config.output_model}\")\n",
    "    print(f\"\\nðŸ“ All plots and reports saved to: {Config.plot_dir}/\")\n",
    "    return study, final_history, best_params, final_evaluation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study, final_history, best_params, final_evaluation = run_advanced_hyperparameter_optimization()\n",
    "    print(\"ðŸŽ‰ Advanced Hyperparameter Optimization with Comprehensive Analysis Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c9a6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in fine tuning Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"in fine tuning Device: \", device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
